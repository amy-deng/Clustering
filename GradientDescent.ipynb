{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Songgaojun Deng 10442772\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "filename = 'data1'\n",
    "\n",
    "df = pd.read_csv(filename,header = None) # 569 rows × 32 columns\n",
    "\n",
    "# Missing attribute values: none\n",
    "\n",
    "### data processing\n",
    "\n",
    "# Drop the first column ”id” which has no meaning for model training.\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df.rename(columns = {1:'class'}, inplace = True)\n",
    "\n",
    "#Set class label malignant positive 1 and benigh negative 0 for more convenient operations.\n",
    "df.loc[df['class'] == 'M', 'class'] = 1 # malignant \n",
    "df.loc[df['class'] == 'B', 'class'] = 0 # \n",
    "\n",
    "# Divide dataset into training 0.8 and testing 0.2 sets randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.values[:,1:], df.values[:,0], test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "#Determine the dimention of features and then initialize feature vector theta based on Normal Distribution with a mean of 0 and a standard deviation of 1.\n",
    "M,N = df.values[:,1:].shape # M : number of samples, N : number of features\n",
    "init_theta = np.random.normal(0, 1, N)\n",
    "\n",
    "#Because K-Fold validation is used, so I set the number of folds in advance.\n",
    "kfold = KFold(8, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "# averaged loss value\n",
    "def avg_logistic_loss(hypothesis, y):\n",
    "    return (- y * np.log(hypothesis) * np.log(1.0-hypothesis)).mean()\n",
    "\n",
    "# calculate the loss and gradient of feature\n",
    "def evaluate_gradient(loss, X,y, theta):\n",
    "    m = len(y) if y is list else 1\n",
    "    XT = X.transpose()\n",
    "    hypothesis = sigmoid(np.dot(X,theta))\n",
    "    avg_loss = loss(hypothesis, y)\n",
    "    \n",
    "    gradient = np.dot(XT, (hypothesis - y))/ m * 1.0\n",
    "    return gradient, avg_loss\n",
    "\n",
    "def predict(theta, X):\n",
    "    return sigmoid(np.dot(X,theta))\n",
    "\n",
    "def accuracy(pred, true):\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == true[i] :\n",
    "            correct += 1\n",
    "    accuracy = (correct)*1.0/ len(pred)\n",
    "    return accuracy\n",
    "\n",
    "# return recall, precision and accuracy\n",
    "def evaluate(pred, true):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == 1 and true[i] ==1:\n",
    "            TP += 1\n",
    "        elif pred[i] == 0 and true[i] ==1:\n",
    "            FN += 1\n",
    "        elif pred[i] == 1 and true[i] ==0:\n",
    "            FP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    recall = TP*1.0/(TP+FN)\n",
    "    precision = TP*1.0/(TP+FP)\n",
    "    accuracy = (TP+TN)*1.0/(TP+FN+FP+TN)\n",
    "    return recall, precision, accuracy\n",
    "\n",
    "def stochastic_gradient_descent(X,y, theta, alpha = 0.0001, epoch = 100):\n",
    "    for e in range(epoch):\n",
    "        X, y = shuffle(X, y)\n",
    "        for i in range(len(y)):\n",
    "            gradient, avg_loss = evaluate_gradient(avg_logistic_loss,X[i],y[i],theta)\n",
    "            theta -= alpha * gradient\n",
    "    return theta\n",
    "\n",
    "def mini_gradient_descent(X,y, theta, alpha = 0.0001, batch_size=20, epoch = 100):\n",
    "    m,n = X.shape\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        for i in range(int(m/batch_size)):\n",
    "            X, y = shuffle(X, y,n_samples = batch_size)\n",
    "            gradient, avg_loss = evaluate_gradient(avg_logistic_loss,X,y,theta)\n",
    "            theta -= alpha * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRADIENT_DESCENT_TRAINING(gradient_algor, X_train, y_train, X_test, y_test, theta):\n",
    "    print gradient_algor.__name__\n",
    "    \n",
    "    alpha_list = []\n",
    "    for i in range(15):\n",
    "        alpha_list.append(i*0.01)\n",
    "    val_acc_dict = {}\n",
    "    val_acc_list = []\n",
    "    theta_dict = {} # save trained theta model\n",
    "    data = df       # training data\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "\n",
    "        for train, val in kfold.split(X_train):\n",
    "            # training\n",
    "            theta = gradient_algor(X_train[train], y_train[train], theta, alpha)\n",
    "            theta_dict[alpha] = theta\n",
    "            pred =  predict(theta, X_train[val])\n",
    "            acc = accuracy(pred, y_train[val])\n",
    "            val_acc_list.append(acc)\n",
    "        val_acc_dict[alpha] = np.mean(val_acc_list)\n",
    "        print(\"alpha %f \\t accuracy %f\" % (alpha, acc))   \n",
    "    \n",
    "    # select model\n",
    "    alpha = max(val_acc_dict, key=val_acc_dict.get)\n",
    "    theta = theta_dict[alpha]\n",
    "    \n",
    "    # test model\n",
    "    recall, precision, acc = evaluate(predict(theta,X_test),y_test)\n",
    "    print\n",
    "    print(\"selected alpha %f \\t recall %f \\t precision %f \\t accuracy %f\" % (alpha, recall, precision, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_gradient_descent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.000000 \t accuracy 0.392857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.010000 \t accuracy 0.553571\n",
      "alpha 0.020000 \t accuracy 0.875000\n",
      "alpha 0.030000 \t accuracy 0.892857\n",
      "alpha 0.040000 \t accuracy 0.839286\n",
      "alpha 0.050000 \t accuracy 0.892857\n",
      "alpha 0.060000 \t accuracy 0.928571\n",
      "alpha 0.070000 \t accuracy 0.892857\n",
      "alpha 0.080000 \t accuracy 0.839286\n",
      "alpha 0.090000 \t accuracy 0.821429\n",
      "alpha 0.100000 \t accuracy 0.857143\n",
      "alpha 0.110000 \t accuracy 0.892857\n",
      "alpha 0.120000 \t accuracy 0.875000\n",
      "alpha 0.130000 \t accuracy 0.892857\n",
      "alpha 0.140000 \t accuracy 0.875000\n",
      "\n",
      "selected alpha 0.140000 \t recall 0.904762 \t precision 0.791667 \t accuracy 0.877193\n"
     ]
    }
   ],
   "source": [
    "# call mini_gradient_descent and report results\n",
    "GRADIENT_DESCENT_TRAINING(mini_gradient_descent, X_train, y_train, X_test, y_test, init_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stochastic_gradient_descent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.000000 \t accuracy 0.875000\n",
      "alpha 0.010000 \t accuracy 0.892857\n",
      "alpha 0.020000 \t accuracy 0.875000\n",
      "alpha 0.030000 \t accuracy 0.892857\n",
      "alpha 0.040000 \t accuracy 0.892857\n",
      "alpha 0.050000 \t accuracy 0.892857\n",
      "alpha 0.060000 \t accuracy 0.839286\n",
      "alpha 0.070000 \t accuracy 0.910714\n",
      "alpha 0.080000 \t accuracy 0.892857\n",
      "alpha 0.090000 \t accuracy 0.892857\n",
      "alpha 0.100000 \t accuracy 0.875000\n",
      "alpha 0.110000 \t accuracy 0.910714\n",
      "alpha 0.120000 \t accuracy 0.910714\n",
      "alpha 0.130000 \t accuracy 0.839286\n",
      "alpha 0.140000 \t accuracy 0.857143\n",
      "\n",
      "selected alpha 0.080000 \t recall 1.000000 \t precision 0.777778 \t accuracy 0.894737\n"
     ]
    }
   ],
   "source": [
    "# call stochastic_gradient_descent and report results\n",
    "GRADIENT_DESCENT_TRAINING(stochastic_gradient_descent, X_train, y_train, X_test, y_test, init_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
